{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9504eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.4.24-cp310-cp310-win_amd64.whl (262 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tqdm, pyyaml, filelock, tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.7.0 pyyaml-6.0 regex-2022.4.24 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77f71bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7000859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from rouge-score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from rouge-score) (1.22.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk->rouge-score) (2022.4.24)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk->rouge-score) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk->rouge-score) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from click->nltk->rouge-score) (0.4.4)\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dcd28d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (4.18.0)\n",
      "Requirement already satisfied: requests in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (2.22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.5.1)\n",
      "Requirement already satisfied: sacremoses in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.0.53)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.12.1)\n",
      "Requirement already satisfied: filelock in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (2022.4.24)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (4.36.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (5.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: protobuf; extra == \"sentencepiece\" in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.11.4)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91; extra == \"sentencepiece\" (from transformers[sentencepiece])\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b4/61c100a53fa950fda87a8b9cd16ff7e9b8f41b9e47e6bd5e7775288cf742/sentencepiece-0.1.96-cp37-cp37m-macosx_10_6_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (3.7.4.3)\n",
      "Requirement already satisfied: joblib in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[sentencepiece]) (0.13.2)\n",
      "Requirement already satisfied: click in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[sentencepiece]) (7.0)\n",
      "Requirement already satisfied: six in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers[sentencepiece]) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers[sentencepiece]) (0.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.2)\n",
      "Requirement already satisfied: setuptools in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from protobuf; extra == \"sentencepiece\"->transformers[sentencepiece]) (41.4.0)\n",
      "Requirement already satisfied: more-itertools in /Users/hakanmeva/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers[sentencepiece]) (7.2.0)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9347737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92521742",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ohmeow-blurr -q\n",
    "!pip install bert-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab8f5f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blurr\n",
      "  Downloading blurr-0.4.1-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.9/52.9 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages (from blurr) (2.8.2)\n",
      "Requirement already satisfied: smart-open in c:\\users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages (from blurr) (5.2.1)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.23.9-py3-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.5/132.5 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pyyaml in c:\\users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages (from blurr) (6.0)\n",
      "Collecting botocore<1.27.0,>=1.26.9\n",
      "  Downloading botocore-1.26.10-py3-none-any.whl (8.8 MB)\n",
      "     ---------------------------------------- 8.8/8.8 MB 35.0 MB/s eta 0:00:00\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.5/79.5 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages (from python-dateutil->blurr) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages (from botocore<1.27.0,>=1.26.9->boto3->blurr) (1.26.9)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=203683717b8706bfd54e158e787ec5d90c4c16433a58892ae2094d3e27f72f18\n",
      "  Stored in directory: c:\\users\\pc\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, jmespath, botocore, s3transfer, boto3, blurr\n",
      "Successfully installed blurr-0.4.1 boto3-1.23.9 botocore-1.26.10 docopt-0.6.2 jmespath-1.0.0 s3transfer-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install blurr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bef283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from sacremoses) (2022.4.24)\n",
      "Requirement already satisfied: six in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from sacremoses) (1.16.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from sacremoses) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from click->sacremoses) (0.4.4)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2653e8a2a741db1952812f3e5e2055a4c0fd090349cb20a6680ff4d9a2801f70\n",
      "  Stored in directory: c:\\users\\pc\\appdata\\local\\pip\\cache\\wheels\\00\\24\\97\\a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: joblib, click, sacremoses\n",
      "Successfully installed click-8.1.3 joblib-1.1.0 sacremoses-0.0.53\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cf9b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.8.9\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0545ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting protobuf<4.0dev,>=3.12.0\n",
      "  Downloading protobuf-3.20.1-cp310-cp310-win_amd64.whl (903 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp310-cp310-win_amd64.whl (10 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (61.2.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting psutil>=5.0.0\n",
      "  Downloading psutil-5.9.1-cp310-cp310-win_amd64.whl (245 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=d1eae4fa731cceacf0fa535a2c14291482c1d239fe949982a57fdab22e49b59f\n",
      "  Stored in directory: c:\\users\\pc\\appdata\\local\\pip\\cache\\wheels\\54\\4e\\28\\3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=01d9fa4064a9ca6e3b3116769be472cc06c31361e396ca902fb62bee44db9017\n",
      "  Stored in directory: c:\\users\\pc\\appdata\\local\\pip\\cache\\wheels\\e7\\f3\\22\\152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 protobuf-3.20.1 psutil-5.9.1 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.17\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8500a581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (21.3)\n",
      "Collecting dill<0.3.5\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (0.7.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp310-cp310-win_amd64.whl (17.9 MB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.2-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp310-cp310-win_amd64.whl (555 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp310-cp310-win_amd64.whl (122 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp310-cp310-win_amd64.whl (27 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, pytz, fsspec, dill, aiohttp, xxhash, responses, pyarrow, pandas, multiprocess, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 multiprocess-0.70.12.2 pandas-1.4.2 pyarrow-8.0.0 pytz-2022.1 responses-0.18.0 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6e7e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4cec666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six in c:\\users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages (from absl-py) (1.16.0)\n",
      "Installing collected packages: absl-py\n",
      "Successfully installed absl-py-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install absl-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe79398",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620ab363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3713244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import SummarizationPipeline\n",
    "import re\n",
    "from rouge import Rouge\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f669a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a6b165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b0e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from tabulate import tabulate\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807f0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062c39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8354c",
   "metadata": {},
   "source": [
    "# Milestone 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e6cd1f",
   "metadata": {},
   "source": [
    "## Import Multinews Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29979f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('multinews_dataset/test_articles.txt', encoding=\"utf8\") as f:\n",
    "    multinews_articles = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4232b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('multinews_dataset/test_summ.txt', encoding=\"utf8\") as f:\n",
    "    multinews_summ = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a81e4",
   "metadata": {},
   "source": [
    "## Clean datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0445e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_article(article):\n",
    "    #keep only alphanumeric values\n",
    "    alphanum_article = re.sub(rf'[^a-z0-9 \\'\\.]', ' ', article)\n",
    "    #delete unwanted white spaces\n",
    "    whitespace_mid = re.sub(rf'\\b( +)\\b', ' ', alphanum_article)\n",
    "    whitespace_end = re.sub(rf'( +)$', '', whitespace_mid)\n",
    "    whitespace_beg = re.sub(rf'^( +)', '', whitespace_end)\n",
    "    \n",
    "    return whitespace_beg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654cf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postproc_article(article):\n",
    "    return re.sub(rf'(\\.)', '', article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91437f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a619901",
   "metadata": {},
   "source": [
    "## Get mean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b644ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_scores(scores):\n",
    "    rouge1_r = []\n",
    "    rouge1_p = []\n",
    "    rouge1_f = []\n",
    "    rouge2_r = []\n",
    "    rouge2_p = []\n",
    "    rouge2_f = []\n",
    "    rougel_r = []\n",
    "    rougel_p = []\n",
    "    rougel_f = []\n",
    "    for score in scores:\n",
    "        rouge1_r.append(score[0]['rouge-1']['r'])\n",
    "        rouge1_p.append(score[0]['rouge-1']['p'])\n",
    "        rouge1_f.append(score[0]['rouge-1']['f'])\n",
    "        \n",
    "        rouge2_r.append(score[0]['rouge-2']['r'])\n",
    "        rouge2_p.append(score[0]['rouge-2']['p'])\n",
    "        rouge2_f.append(score[0]['rouge-2']['f'])\n",
    "        \n",
    "        rougel_r.append(score[0]['rouge-l']['r'])\n",
    "        rougel_p.append(score[0]['rouge-l']['p'])\n",
    "        rougel_f.append(score[0]['rouge-l']['f'])\n",
    "    \n",
    "    results = [np.mean(rouge1_r), np.mean(rouge1_p), np.mean(rouge1_f),\n",
    "               np.mean(rouge2_r), np.mean(rouge2_p), np.mean(rouge2_f),\n",
    "               np.mean(rougel_r), np.mean(rougel_p), np.mean(rougel_f)]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad73392",
   "metadata": {},
   "source": [
    "## Define BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "58b04400",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn', tokenizer='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd7324",
   "metadata": {},
   "source": [
    "## BART on Multinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccdbfb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 149. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n",
      "Your max_length is set to 250, but you input_length is only 236. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=118)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 230. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=115)\n",
      "Your max_length is set to 250, but you input_length is only 224. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2272 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         if (\n\u001b[1;32m    139\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;31m# and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m-> 1157\u001b[0;31m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             )\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_shape, past_key_values_length)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         )\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_bart_multinews = []\n",
    "for idx, article in enumerate(multinews_articles[:5000]):\n",
    "    if idx % 100 == 0:\n",
    "        print(idx)\n",
    "    generated_summary = summarizer(article, min_length = 50, max_length = 250, do_sample=False)\n",
    "    gen_summ = postproc_article(\n",
    "                   clean_article(\n",
    "                       generated_summary[0]['summary_text'].lower()))\n",
    "    \n",
    "    target_summ = postproc_article(\n",
    "                      clean_article(multinews_summ[idx]))\n",
    "    \n",
    "    scores_bart_multinews.append(rouge.get_scores(gen_summ, target_summ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efc83ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bart_multinews = get_mean_scores(scores_bart_multinews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3e3fd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19398086374359405,\n",
       " 0.566394042529543,\n",
       " 0.2837468827696245,\n",
       " 0.06462788884844443,\n",
       " 0.2296533666107905,\n",
       " 0.09864009538725348,\n",
       " 0.12560604791652127,\n",
       " 0.36465727716591695,\n",
       " 0.18340505161625237]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bart_multinews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6562aa78",
   "metadata": {},
   "source": [
    "## BART on NewsRoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52bda5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsroom_dataset = []\n",
    "with open('newsroom_dataset/test.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    newsroom_dataset.append(json.loads(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17029040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 74. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 70. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "11\n",
      "12\n",
      "13\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "37\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 249. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=124)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 154. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "61\n",
      "62\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 228. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "65\n",
      "68\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 216. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "72\n",
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 200. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "77\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 135. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=67)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "87\n",
      "90\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 73. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 241. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 170. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 101. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 108. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 150. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 136. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 127. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 226. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "133\n",
      "134\n",
      "136\n",
      "137\n",
      "138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 228. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "145\n",
      "147\n",
      "148\n",
      "149\n",
      "152\n",
      "154\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "161\n",
      "162\n",
      "164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "166\n",
      "167\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 241. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "181\n",
      "182\n",
      "185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 198. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=99)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 203. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "190\n",
      "191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 214. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=107)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 75. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 149. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=74)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 92. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 174. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 172. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "205\n",
      "206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 206. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "209\n",
      "210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 152. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=76)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "212\n",
      "213\n",
      "215\n",
      "218\n",
      "219\n",
      "221\n",
      "222\n",
      "223\n",
      "225\n",
      "226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 74. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n",
      "229\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 215. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=107)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n",
      "242\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 244. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=122)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "250\n",
      "251\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "260\n",
      "261\n",
      "263\n",
      "264\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "277\n",
      "279\n",
      "281\n",
      "283\n",
      "284\n",
      "287\n",
      "289\n",
      "290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 184. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=92)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 237. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=118)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "299\n",
      "CPU times: user 44min 6s, sys: 5min 19s, total: 49min 25s\n",
      "Wall time: 45min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_bart_newsroom = []\n",
    "for idx, article in enumerate(newsroom_dataset[:5000]):\n",
    "    if len(article['text']) < 5000:\n",
    "        print(idx)\n",
    "        generated_summary = summarizer(article['text'].lower(), min_length = 50, max_length = 250, do_sample=False)\n",
    "        gen_summ = postproc_article(\n",
    "                   clean_article(\n",
    "                       generated_summary[0]['summary_text'].lower()))\n",
    "    \n",
    "        target_summ = postproc_article(\n",
    "                      clean_article(article['summary'].lower()))\n",
    "    \n",
    "        scores_bart_newsroom.append(rouge.get_scores(gen_summ, target_summ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c8692a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/bart_newsroom', 'wb') as fp:\n",
    "    pickle.dump(scores_bart_newsroom, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "832da209",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bart_newsroom = get_mean_scores(scores_bart_newsroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca64a10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3886622495938386,\n",
       " 0.2133014910052308,\n",
       " 0.25999745918996875,\n",
       " 0.17274045629097223,\n",
       " 0.10011144012944892,\n",
       " 0.11693256036895888,\n",
       " 0.3177664210377903,\n",
       " 0.17658347124434656,\n",
       " 0.21367429518976228]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bart_newsroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409244c",
   "metadata": {},
   "source": [
    "## Define MBART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bde8d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFMBartForConditionalGeneration: ['lm_head.weight']\n",
      "- This IS expected if you are initializing TFMBartForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFMBartForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFMBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"ahmeddbahaa/mbart-large-50-finetuned-ar-wikilingua\")\n",
    "loaded_model = TFAutoModelForSeq2SeqLM.from_pretrained(\"ahmeddbahaa/mbart-large-50-finetuned-ar-wikilingua\", from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f37f5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbart = SummarizationPipeline(model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0b0c8",
   "metadata": {},
   "source": [
    "## MBART on Multinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7114b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 169. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=84)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 247. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Hypothesis is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rouge/rouge.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_avg_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rouge/rouge.py\u001b[0m in \u001b[0;36m_get_scores\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0mraw_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     exclusive=self.exclusive)\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0msen_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rouge/rouge.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(hyp, ref, **k)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mDEFAULT_METRICS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rouge-2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rouge-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     AVAILABLE_METRICS = {\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;34m\"rouge-1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;34m\"rouge-2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"rouge-3\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/rouge/rouge_score.py\u001b[0m in \u001b[0;36mrouge_n\u001b[0;34m(evaluated_sentences, reference_sentences, n, raw_results, exclusive)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluated_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hypothesis is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reference is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Hypothesis is empty."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_mbart_multinews2 = []\n",
    "for idx, article in enumerate(multinews_articles[:5000]):\n",
    "    print(idx)\n",
    "    generated_summary = mbart(multinews_articles[idx].lower(), min_length = 50, max_length = 250, do_sample = False)\n",
    "    gen_summ = postproc_article(\n",
    "                   clean_article(\n",
    "                       generated_summary[0]['summary_text'].lower()))\n",
    "    \n",
    "    target_summ = postproc_article(\n",
    "                      clean_article(multinews_summ[idx]))\n",
    "    \n",
    "    scores_mbart_multinews2.append(rouge.get_scores(gen_summ, target_summ)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f90f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         if (\n\u001b[1;32m    139\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                 \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                 \u001b[0minference_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mforced_eos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforced_eos_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36m_generate_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, num_return_sequences, length_penalty, num_beams, vocab_size, encoder_outputs, attention_mask, use_cache, forced_bos_token_id, forced_eos_token_id, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_attentions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             )\n\u001b[1;32m    913\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (batch_size * num_beams, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         )\n\u001b[1;32m   1327\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m         )\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcross_attn_layer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_head_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcross_attn_head_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m             )\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, training)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         dtype=self._compute_dtype_object)\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/ops/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;31m# Broadcast kernel to inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Reshape the output back to the original ndim of the input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes, name)\u001b[0m\n\u001b[1;32m   4800\u001b[0m     b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\n\u001b[1;32m   4801\u001b[0m         b, b_axes, True)\n\u001b[0;32m-> 4802\u001b[0;31m     \u001b[0mab_matmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_reshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4803\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_free_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4804\u001b[0m       if (ab_matmul.get_shape().is_fully_defined() and\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3488\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3490\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5695\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   5696\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5697\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5698\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5699\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_mbart_multinews2 = []\n",
    "for idx, article in enumerate(multinews_articles[:5000]):\n",
    "    generated_summary = mbart(multinews_articles[idx].lower(), min_length = 50, max_length = 250, do_sample = False)\n",
    "    gen_summ = postproc_article(\n",
    "                   clean_article(\n",
    "                       generated_summary[0]['summary_text'].lower()))\n",
    "    \n",
    "    target_summ = postproc_article(\n",
    "                      clean_article(multinews_summ[idx]))\n",
    "    \n",
    "    scores_mbart_multinews2.append(rouge.get_scores(gen_summ, target_summ)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61008e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mbart_multinews.append(scores_mbart_multinews2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14ba85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in scores_mbart_multinews2:\n",
    "    scores_mbart_multinews.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56642824",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/mbart_multinews', 'wb') as fp:\n",
    "    pickle.dump(scores_mbart_multinews, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e413dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mbart_multinews = get_mean_scores(scores_mbart_multinews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b6b330d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15190895015885073,\n",
       " 0.48878609433962067,\n",
       " 0.22636914634400293,\n",
       " 0.038035467795156756,\n",
       " 0.14922600079869042,\n",
       " 0.05854957294613635,\n",
       " 0.09400774399952658,\n",
       " 0.3040036051022214,\n",
       " 0.1401548530546241]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mbart_multinews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c173b22",
   "metadata": {},
   "source": [
    "## MBART on NewsRoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15286a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "11\n",
      "12\n",
      "13\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "37\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 155. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "61\n",
      "62\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 229. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "65\n",
      "68\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 208. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=104)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "72\n",
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 204. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=102)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "77\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 144. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "87\n",
      "90\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 71. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 227. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=113)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 182. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 115. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 107. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 158. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=79)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 154. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=77)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 141. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 230. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=115)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "133\n",
      "134\n",
      "136\n",
      "137\n",
      "138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 235. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=117)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "145\n",
      "147\n",
      "148\n",
      "149\n",
      "152\n",
      "154\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "161\n",
      "162\n",
      "164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 245. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=122)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "166\n",
      "167\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "181\n",
      "182\n",
      "185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 213. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=106)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 182. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=91)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "190\n",
      "191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 224. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 85. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 165. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=82)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 95. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 174. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 174. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=87)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "205\n",
      "206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 229. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=114)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "209\n",
      "210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 163. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "212\n",
      "213\n",
      "215\n",
      "218\n",
      "219\n",
      "221\n",
      "222\n",
      "223\n",
      "225\n",
      "226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 74. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n",
      "229\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 237. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=118)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n",
      "242\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "250\n",
      "251\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "260\n",
      "261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 233. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=116)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "264\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "277\n",
      "279\n",
      "281\n",
      "283\n",
      "284\n",
      "287\n",
      "289\n",
      "290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 195. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=97)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but you input_length is only 247. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "299\n",
      "300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         if (\n\u001b[1;32m    139\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                 \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                 \u001b[0minference_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mforced_eos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforced_eos_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_tf_utils.py\u001b[0m in \u001b[0;36m_generate_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, early_stopping, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, num_return_sequences, length_penalty, num_beams, vocab_size, encoder_outputs, attention_mask, use_cache, forced_bos_token_id, forced_eos_token_id, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_attentions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             )\n\u001b[1;32m    913\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (batch_size * num_beams, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         )\n\u001b[1;32m   1327\u001b[0m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m         )\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mmain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcross_attn_layer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_attn_head_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcross_attn_head_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                 \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m             )\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, training)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/mbart/modeling_tf_mbart.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, training)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# reuse k, v, self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1032\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1033\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         dtype=self._compute_dtype_object)\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/ops/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, kernel, bias, activation, dtype)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3376\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3377\u001b[0m       return gen_nn_ops.bias_add(\n\u001b[0;32m-> 3378\u001b[0;31m           value, bias, data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m   3379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 675\u001b[0;31m         _ctx, \"BiasAdd\", name, value, bias, \"data_format\", data_format)\n\u001b[0m\u001b[1;32m    676\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_mbart_newsroom = []\n",
    "for idx, article in enumerate(newsroom_dataset[:5000]):\n",
    "    if len(article['text']) < 4000:\n",
    "        print(idx)\n",
    "        generated_summary = mbart(article['text'].lower(), min_length = 50, max_length = 250, do_sample=False)\n",
    "        gen_summ = postproc_article(\n",
    "                   clean_article(\n",
    "                       generated_summary[0]['summary_text'].lower()))\n",
    "    \n",
    "        target_summ = postproc_article(\n",
    "                      clean_article(article['summary'].lower()))\n",
    "    \n",
    "        scores_mbart_newsroom.append(rouge.get_scores(gen_summ, target_summ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "83c3d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/mbart_newsroom', 'wb') as fp:\n",
    "    pickle.dump(scores_mbart_newsroom, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9609c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mbart_newsroom = get_mean_scores(scores_mbart_newsroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fa02201d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42353486766787923,\n",
       " 0.26831545664820483,\n",
       " 0.3039959184876281,\n",
       " 0.2283095711836132,\n",
       " 0.15472137820969037,\n",
       " 0.16788760038538053,\n",
       " 0.3624112544416273,\n",
       " 0.23556015145056014,\n",
       " 0.26331424199200837]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_mbart_newsroom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b15da1",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "07af47d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBartForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['final_logits_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "demo_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "demo_model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn', from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4fa596e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"gop eyes gains as voters in 11 states pick governors enlarge this image toggle caption jim cole/ap jim cole/ap voters in 11 states will pick their governors tonight , and republicans appear on track to increase their numbers by at least one , with the potential to extend their hold to more than two-thirds of the nation ' s top state offices . eight of the gubernatorial seats up for grabs are now held by democrats ; three are in republican hands . republicans currently hold 29 governorships , democrats have 20 , and rhode island ' s gov . lincoln chafee is an independent . polls and race analysts suggest that only three of tonight ' s contests are considered competitive , all in states where incumbent democratic governors aren ' t running again : montana , new hampshire and washington . while those state races remain too close to call , republicans are expected to wrest the north carolina governorship from democratic control , and to easily win gop-held seats in utah , north dakota and indiana . democrats are likely to hold on to their seats in west virginia and missouri , and are expected to notch safe wins in races for seats they hold in vermont and delaware . holding sway on health care while the occupant of the governor ' s office is historically far less important than the party that controls the state legislature , top state officials in coming years are expected to story_separator_special_tag gop eyes gains as voters in 11 states pick governors jim cole / ap i jim cole / ap voters in 11 states will pick their governors tonight , and republicans appear on track to increase their numbers by at least one , and with the potential to extend their hold to more than two-thirds of the nation ' s top state offices . eight of the gubernatorial seats up for grabs today are now held by democrats ; three are in republican hands . republicans currently hold 29 governorships , democrats have 20 ; and rhode island ' s gov . lincoln chafee is an independent . polls and race analysts suggest that only three of tonight ' s contests are considered competitive , all in states where incumbent democratic governors aren ' t running again : montana , new hampshire and washington . while those state races remain too close to call , republicans are expected to wrest the north carolina governorship from democratic control , and to easily win gop-held seats in utah , north dakota and indiana . democrats are likely hold on to their seats in west virginia and missouri ; and expected to notch safe wins in races for seats they hold in vermont and delaware . holding sway on health care while the occupant of the governor ' s office is historically far less important than the party that controls the state legislature , top state officials in coming years are expected to\\n\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinews_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "30a749df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 571, 1517, 2473, 3077, 25, 1983, 11, 365, 982, 1339, 12066, 22368, 42, 2274, 23012, 3747, 1236, 757, 1029, 459, 6256, 1236, 757, 1029, 459, 6256, 1983, 11, 365, 982, 40, 1339, 49, 12066, 3422, 8, 16441, 1253, 2082, 15, 1349, 7, 712, 49, 1530, 30, 23, 513, 65, 19, 5, 801, 7, 4442, 49, 946, 7, 55, 87, 80, 29193, 9, 5, 1226, 128, 579, 299, 194, 4088, 479, 799, 9, 5, 18932, 3202, 62, 13, 17565, 32, 122, 547, 30, 38538, 2923, 130, 32, 11, 37958, 1420, 479, 16441, 1253, 855, 946, 1132, 12066, 7903, 38538, 2923, 33, 291, 8, 16448, 4636, 2946, 128, 579, 213, 705, 479, 784, 37654, 1855, 2001, 1942, 16, 41, 2222, 479, 4583, 8, 1015, 1066, 3608, 14, 129, 130, 9, 3422, 128, 579, 11997, 32, 1687, 2695, 70, 11, 982, 147, 9861, 7368, 12066, 2025, 128, 326, 878, 456, 27121, 1113, 92, 1368, 3914, 9959, 8, 14784, 1054, 479, 150, 167, 194, 4694, 1091, 350, 593, 7, 486, 16441, 1253, 32, 421, 7, 22160, 5, 1926, 512, 27343, 11738, 11887, 31, 7368, 797, 8, 7, 2773, 339, 821, 1517, 547, 3202, 11, 16080, 895, 1926, 385, 677, 6804, 8, 9473, 8878, 479, 38538, 2923, 32, 533, 7, 946, 15, 7, 49, 3202, 11, 3072, 33799, 493, 8, 2649, 22805, 8, 32, 421, 7, 16046, 1522, 2693, 11, 4694, 13, 3202, 51, 946, 11, 4342, 7861, 8, 2424, 24590, 479, 1826, 17980, 15, 474, 575, 150, 5, 31951, 9, 5, 2318, 128, 579, 558, 16, 9644, 444, 540, 505, 87, 5, 537, 14, 5656, 5, 194, 9251, 299, 194, 503, 11, 567, 107, 32, 421, 7, 527, 31207, 2630, 780, 6694, 821, 1517, 2473, 3077, 25, 1983, 11, 365, 982, 1339, 12066, 1236, 757, 1029, 459, 6256, 939, 1236, 757, 1029, 459, 6256, 1983, 11, 365, 982, 40, 1339, 49, 12066, 3422, 8, 16441, 1253, 2082, 15, 1349, 7, 712, 49, 1530, 30, 23, 513, 65, 8, 19, 5, 801, 7, 4442, 49, 946, 7, 55, 87, 80, 29193, 9, 5, 1226, 128, 579, 299, 194, 4088, 479, 799, 9, 5, 18932, 3202, 62, 13, 17565, 452, 32, 122, 547, 30, 38538, 2923, 130, 32, 11, 37958, 1420, 479, 16441, 1253, 855, 946, 1132, 12066, 7903, 38538, 2923, 33, 291, 8, 16448, 4636, 2946, 128, 579, 213, 705, 479, 784, 37654, 1855, 2001, 1942, 16, 41, 2222, 479, 4583, 8, 1015, 1066, 3608, 14, 129, 130, 9, 3422, 128, 579, 11997, 32, 1687, 2695, 70, 11, 982, 147, 9861, 7368, 12066, 2025, 128, 326, 878, 456, 27121, 1113, 92, 1368, 3914, 9959, 8, 14784, 1054, 479, 150, 167, 194, 4694, 1091, 350, 593, 7, 486, 16441, 1253, 32, 421, 7, 22160, 5, 1926, 512, 27343, 11738, 11887, 31, 7368, 797, 8, 7, 2773, 339, 821, 1517, 547, 3202, 11, 16080, 895, 1926, 385, 677, 6804, 8, 9473, 8878, 479, 38538, 2923, 32, 533, 946, 15, 7, 49, 3202, 11, 3072, 33799, 493, 8, 2649, 22805, 8, 421, 7, 16046, 1522, 2693, 11, 4694, 13, 3202, 51, 946, 11, 4342, 7861, 8, 2424, 24590, 479, 1826, 17980, 15, 474, 575, 150, 5, 31951, 9, 5, 2318, 128, 579, 558, 16, 9644, 444, 540, 505, 87, 5, 537, 14, 5656, 5, 194, 9251, 299, 194, 503, 11, 567, 107, 32, 421, 7, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_tokenizer(clean_article(multinews_articles[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9ea87d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn', tokenizer='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "779b6239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"Voters in 11 states will pick their governors tonight. Eight of the gubernatorial seats up for grabs are now held by democrats three are in republican hands. republicans currently hold 29 governorships democrats have 20 and rhode island ' s gov . lincoln chafee is an independent.\"}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(clean_article(multinews_articles[0]), min_length = 50, max_length = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42dc565",
   "metadata": {},
   "source": [
    " # Final milestone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7d1fc",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35db3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"document\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da516657",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fa1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6f1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3416e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ahmeddbahaa/mbart-large-50-finetuned-ar-wikilingua\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "182416ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/mbart-large-50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a457ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/mbart-large-cc25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb52fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sshleifer/tiny-mbart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874fa365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/mbart-large-en-ro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af98702",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90db9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length = 256\n",
    "decoder_max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69930610",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "357c1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba29e6f",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a84298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration english\n",
      "Reusing dataset multi_news (C:\\Users\\pc\\.cache\\huggingface\\datasets\\multi_news\\english\\1.0.0\\04f6b32c1fe246d12beab94a36e0752c3ee43e024afb9edd1e6bcc87e1457b45)\n"
     ]
    }
   ],
   "source": [
    "# multinews dataset found in datasets module\n",
    "data = datasets.load_dataset(\"multi_news\", name=language, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23125fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary'],\n",
       "    num_rows: 5622\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d47fdb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6bde83e8a6e31240\n",
      "Reusing dataset json (C:\\Users\\pc\\.cache\\huggingface\\datasets\\json\\default-6bde83e8a6e31240\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c16617da70f40fe843a025e845e0bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#newshead dataset imported from local file\n",
    "data = load_dataset('json', data_files = 'newsroom_dataset/test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39798cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'archive', 'title', 'date', 'text', 'summary', 'compression', 'coverage', 'density', 'compression_bin', 'coverage_bin', 'density_bin'],\n",
       "        num_rows: 108862\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82a9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3d593",
   "metadata": {},
   "source": [
    "### Train/Test sets utills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a73702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\pc\\.cache\\huggingface\\datasets\\json\\default-6bde83e8a6e31240\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-f018e50869e957ea.arrow\n"
     ]
    }
   ],
   "source": [
    "def flatten(example):\n",
    "    return {\n",
    "        \"document\": example[\"text\"],\n",
    "        \"summary\": example[\"title\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def list2samples(example):\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    for sample in zip(example[\"document\"], example[\"summary\"]):\n",
    "        if len(sample[0]) > 0:\n",
    "            documents += sample[0]\n",
    "            summaries += sample[1]\n",
    "    return {\"document\": documents, \"summary\": summaries}\n",
    "\n",
    "\n",
    "dataset = dataset.map(flatten, remove_columns = ['url', 'archive', 'date',\n",
    "                                               'compression', 'coverage', 'density', \n",
    "                                               'compression_bin', 'coverage_bin', 'density_bin'])\n",
    "#dataset = data.map(list2samples, batched=True)\n",
    "\n",
    "train_data_txt, validation_data_txt = dataset.train_test_split(test_size=0.05).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8b5a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'text', 'summary', 'document'],\n",
       "    num_rows: 5444\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf709d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbb156258c34ec1b6d16f39fada2cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5444 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data = validation_data_txt.map(flatten, remove_columns = ['summary', 'document'])\n",
    "#summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb41d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_txt, validation_data_txt = all_data.train_test_split(test_size=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64061f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e7ba272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf5ace7251d4f489bce12cdba747346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677306b9b884420c81c6eb7da9aeb4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    #source, target = batch[\"document\"], batch[\"summary\"]\n",
    "    #source, target = batch[\"text\"], batch[\"summary\"]\n",
    "    source, target = batch[\"text\"], batch[\"title\"]\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_data = train_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=train_data_txt.column_names,\n",
    ")\n",
    "\n",
    "validation_data = validation_data_txt.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=validation_data_txt.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c280b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 1) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "992ffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=1, \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=1,  \n",
    "    per_device_eval_batch_size=1,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952e39b",
   "metadata": {},
   "source": [
    "## BART on Multinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6e62c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 04:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220528_224255-1jktun8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/1jktun8k\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.3367719650268555,\n",
       " 'eval_rouge1': 30.8261,\n",
       " 'eval_rouge2': 8.2431,\n",
       " 'eval_rougeL': 18.5757,\n",
       " 'eval_rougeLsum': 25.6941,\n",
       " 'eval_gen_len': 68.7709,\n",
       " 'eval_runtime': 290.3749,\n",
       " 'eval_samples_per_second': 1.939,\n",
       " 'eval_steps_per_second': 0.486}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ee6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53de8a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5059\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12650\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220528_225129-2um8pk5h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/2um8pk5h\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12650' max='12650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12650/12650 2:05:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.755100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.891500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.935200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.857500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.924900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.845900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.173800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>3.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.593600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.554200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.628400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>2.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>2.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.611100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>2.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>2.654700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>2.611600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>2.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>2.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>2.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.646300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>2.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>2.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>2.630800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>2.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>2.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>2.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.551500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>2.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>2.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>2.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>2.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>2.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>2.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>2.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>2.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>2.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>2.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>2.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>2.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>2.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>2.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>2.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>2.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>2.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>2.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>2.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>2.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>2.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>2.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>2.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.196200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>2.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>2.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>2.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>2.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>2.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>2.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.880100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.881700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.854300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>1.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>1.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>1.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.928400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>1.867000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-500\n",
      "Configuration saved in results\\checkpoint-500\\config.json\n",
      "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
      "Saving model checkpoint to results\\checkpoint-1000\n",
      "Configuration saved in results\\checkpoint-1000\\config.json\n",
      "Model weights saved in results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to results\\checkpoint-1500\n",
      "Configuration saved in results\\checkpoint-1500\\config.json\n",
      "Model weights saved in results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-1500\\special_tokens_map.json\n",
      "Saving model checkpoint to results\\checkpoint-2000\n",
      "Configuration saved in results\\checkpoint-2000\\config.json\n",
      "Model weights saved in results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-2500\n",
      "Configuration saved in results\\checkpoint-2500\\config.json\n",
      "Model weights saved in results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-3000\n",
      "Configuration saved in results\\checkpoint-3000\\config.json\n",
      "Model weights saved in results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-3500\n",
      "Configuration saved in results\\checkpoint-3500\\config.json\n",
      "Model weights saved in results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-4000\n",
      "Configuration saved in results\\checkpoint-4000\\config.json\n",
      "Model weights saved in results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-4500\n",
      "Configuration saved in results\\checkpoint-4500\\config.json\n",
      "Model weights saved in results\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-5000\n",
      "Configuration saved in results\\checkpoint-5000\\config.json\n",
      "Model weights saved in results\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-5500\n",
      "Configuration saved in results\\checkpoint-5500\\config.json\n",
      "Model weights saved in results\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-6000\n",
      "Configuration saved in results\\checkpoint-6000\\config.json\n",
      "Model weights saved in results\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-6500\n",
      "Configuration saved in results\\checkpoint-6500\\config.json\n",
      "Model weights saved in results\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-7000\n",
      "Configuration saved in results\\checkpoint-7000\\config.json\n",
      "Model weights saved in results\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-7500\n",
      "Configuration saved in results\\checkpoint-7500\\config.json\n",
      "Model weights saved in results\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-7500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-8000\n",
      "Configuration saved in results\\checkpoint-8000\\config.json\n",
      "Model weights saved in results\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-8500\n",
      "Configuration saved in results\\checkpoint-8500\\config.json\n",
      "Model weights saved in results\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-8500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-9000\n",
      "Configuration saved in results\\checkpoint-9000\\config.json\n",
      "Model weights saved in results\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-9000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-9500\n",
      "Configuration saved in results\\checkpoint-9500\\config.json\n",
      "Model weights saved in results\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-9500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-10000\n",
      "Configuration saved in results\\checkpoint-10000\\config.json\n",
      "Model weights saved in results\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-10500\n",
      "Configuration saved in results\\checkpoint-10500\\config.json\n",
      "Model weights saved in results\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-10500\\tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in results\\checkpoint-10500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-11000\n",
      "Configuration saved in results\\checkpoint-11000\\config.json\n",
      "Model weights saved in results\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-11000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-11500\n",
      "Configuration saved in results\\checkpoint-11500\\config.json\n",
      "Model weights saved in results\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-11500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-12000\n",
      "Configuration saved in results\\checkpoint-12000\\config.json\n",
      "Model weights saved in results\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-12500\n",
      "Configuration saved in results\\checkpoint-12500\\config.json\n",
      "Model weights saved in results\\checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-12500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-11000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12650, training_loss=2.729535662986544, metrics={'train_runtime': 7511.7199, 'train_samples_per_second': 3.367, 'train_steps_per_second': 1.684, 'total_flos': 1.370422772760576e+16, 'train_loss': 2.729535662986544, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69649f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 06:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.191427707672119,\n",
       " 'eval_rouge1': 36.42,\n",
       " 'eval_rouge2': 11.91,\n",
       " 'eval_rougeL': 22.72,\n",
       " 'eval_rougeLsum': 29.01,\n",
       " 'eval_gen_len': 65.0,\n",
       " 'eval_runtime': 404.8775,\n",
       " 'eval_samples_per_second': 1.391,\n",
       " 'eval_steps_per_second': 0.697,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37adbe9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/bart-large-cnn/resolve/main/config.json from cache at C:\\Users\\pc/.cache\\huggingface\\transformers\\199ab6c0f28e763098fd3ea09fd68a0928bb297d0f76b9f3375e8a1d652748f9.930264180d256e6fe8e4ba6a728dd80e969493c23d4caa0a6f943614c52d34ab\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/bart-large-cnn/resolve/main/pytorch_model.bin from cache at C:\\Users\\pc/.cache\\huggingface\\transformers\\4ccdf4cdc01b790f9f9c636c7695b5d443180e8dbd0cbe49e07aa918dda1cef0.fa29468c10a34ef7f6cfceba3b174d3ccc95f8d755c3ca1b829aff41cc92a300\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "test_samples = validation_data_txt[0] \n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6fd0596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the crude crept closer to Florida, the risky effort to contain the nation's worst oil spill hit a snag Wednesday when a diamond-edged saw became stuck in a thick pipe on a blown-out well at the bottom of the Gulf. \n",
      "  \n",
      " Coast Guard Adm. Thad Allen said the goal was to free the saw and finish the cut later in the day. This is the latest attempt to contain _ not plug _ the gusher. The best chance at stopping the leak is a relief well, which is at least two months from completion. \n",
      "  \n",
      " \"I don't think the issue is whether or not we can make the second cut. It's about how fine we can make it, how smooth we can make it,\" Allen said. \n",
      "  \n",
      " If the cut is not as smooth as engineers would like, they would be forced to put a looser fitting cap on top of the oil spewing out. This cut-and-cap effort could temporarily increase the flow of oil by as much as 20 percent, though Allen said officials wouldn't know whether that had happened until the cut could be completed. \n",
      "  \n",
      " Engineers may have to bring in a second saw awaiting on a boat, but it was not immediately clear how long that could delay the operation. Live video showed oil spewing out of the new cut, and crews were shooting chemicals to try to disperse the crude. The cap could be placed over the spill as early as Wednesday. \n",
      "  \n",
      " The effort underwater was going on as oil drifted close to the Florida Panhandle's white sand beaches for the first time and investors ran from BP's stock for a second day, reacting to the company's weekend failure to plug the leak by shooting mud and cement into the well, known as the top kill. \n",
      "  \n",
      " Meanwhile, President Barack Obama said in prepared remarks that it was time to roll back billions of dollars in tax breaks for oil companies and use the money for clean energy research and development. He said the catastrophic Gulf oil spill shows the country must move toward clean energy, tapping natural gas and nuclear power and eliminating tax breaks for big oil. \n",
      "  \n",
      " The Justice Department also has announced it started criminal and civil probes into the spill, although the department did not name specific targets for prosecution. \n",
      "  \n",
      " Shares in British-based BP PLC were down 3 percent early Wednesday before recovering in London trading, a day after a 13 percent fall. BP has lost $75 billion in market value since the spill started with an April 20 oil rig explosion and analysts expect damage claims to total billions more. \n",
      "  \n",
      " In Florida, oil was about seven miles south of Pensacola Beach, Allen said. \n",
      "  \n",
      " Thunderstorms were making it difficult to track the slick, Escambia County emergency director John Dosh said, and officials hoped the weather would clear so they could get an aerial view. \n",
      "  \n",
      " It was raining and cloudy at the beach off and on. The four pirate flags on top of Peg Leg Pete's Oyster Bar on the beach where flapping eastward, which could send the oil closer to shore. Peg Leg Pete's is a beach institution frequented by boaters who pull up behind the restaurant in a cove off Pensacola Bay. \n",
      "  \n",
      " It was a slow Memorial Day weekend. Hundreds of tourists enjoyed the turquoise waters, but it wasn't as busy as usual. \n",
      "  \n",
      " \"We are looking at a Wednesday to Friday shoreline impact, but there is a line of uncertainty that depends on the wave action and the winds,\" Dosh said. \n",
      "  \n",
      " \"Today we are in a monitoring mode.\" \n",
      "  \n",
      " Emergency crews began scouring the beaches for oil and shoring up miles of boom, though choppy waters from thunderstorms could send the oil over the protective lines. County officials are using the boom to block oil from reaching inland waterways but plan to leave beaches unprotected because they are easier to clean up. \n",
      "  \n",
      " \"It's inevitable that we will see it on the beaches,\" said Keith Wilkins, deputy chief of neighborhood and community services for Escambia County. \n",
      "  \n",
      " The oil has been spreading in the Gulf since the Deepwater Horizon rig exploded six weeks ago, killing 11 workers and eventually sinking. The rig was being operated for BP, the largest oil and gas producer in the Gulf. \n",
      "  \n",
      " Crude has already been reported along barrier islands in Alabama and Mississippi, and it has polluted some 125 miles of Louisiana coastline. \n",
      "  \n",
      " Some 2,000 feet above the Chandeleur islands off the Louisiana coast, vast stretches of sheen meandered through the water, some reflecting pale rainbow colors. Farther away, swaths of oil was in various sizes and thickness _ from pale, barely detectable streams to rust-rimmed blue and purple patches that looked like giant bruises. \n",
      "  \n",
      " \"The problem is this oil now is really like a collection of smaller spills,\" Allen said during flight in a C-144. \n",
      "  \n",
      " Allen, the national incident commander for the spill, said the threat of oil hitting the coast was shifting east and skimmer vessels would be working offshore to intercept as much crude as possible. \n",
      "  \n",
      " Earlier this week, BP officials said they were concentrating cleanup efforts in Louisiana because they did not expect oil to reach other states. The company has set up floating hotels on barges to house cleanup crews closer to the Louisiana shores. \n",
      "  \n",
      " In Venice, La., hundreds of oil response workers were grounded by storms and many local fishermen hired in the so-called vessels of opportunity program were sent home early. Venice is a major staging center for the oil response, with nearly 1,500 workers and dozens of boats in this small town in the marshes. \n",
      "  \n",
      " \"Boats were staged and ready to go\" in case there was a break in the weather, said BP spokesman Mike Abendoff. \n",
      "  \n",
      " Some work continued, including at a 32-acre site where an \"employee village\" is being built to eventually house as many as 1,500 workers in trailers. Abendoff hoped to start putting some workers there by early next week. \n",
      "  \n",
      " More federal fishing waters were closed, too, another setback for one of the region's most important industries. More than one-third of federal waters were off-limits for fishing, along with hundreds of square miles of state waters. \n",
      "  \n",
      " ___ \n",
      "  \n",
      " Bluestein reported from Covington, La. Associated Press writers Darlene Superville and Pete Yost from Washington, Curt Anderson from Miami, Kevin McGill in Schriever, Melissa Nelson in Pensacola, Fla., Brian Skoloff in Port Fourchon, Mary Foster in Boothville, and Michael Kunzelman in New Orleans also contributed to this report. ||||| I made a hurtful and thoughtless comment on Sunday when I said that \"I wanted my life back.\" When I read that recently, I was appalled. I apologize, especially to the families of the 11 men who lost their lives in this tragic accident. Those words don't represent how I feel about this tragedy, and certainly don't represent the hearts of the people of BP -- many of whom live and work in the Gulf -- who are doing everything they can to make things right. My first priority is doing all we can to restore the lives of the people of the Gulf region and their families -- to restore their lives, not mine. |||||\n"
     ]
    }
   ],
   "source": [
    "print(test_samples['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061b0b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "– British Petroleum CEO Tony Hayward has been roundly bashed for his unfortunate \"I'd like my life back\" comment—lots of fishermen and coastal residents want their lives back, too, not to mention relatives of the 11 men killed in the initial explosion. Today, Hayward apologized, reports the Huffington Post. \"I made a hurtful and thoughtless comment,\" he said. \"When I read that recently, I was appalled. I apologize, especially to the families of the 11 men who lost their lives in this tragic accident.\" His statement continues: \"Those words don't represent how I feel about this tragedy, and certainly don't represent the hearts of the people of BP. My first priority is doing all we can to restore the lives of the people of the Gulf region and their families—to restore their lives, not mine.\" In the meantime, that saw remains stuck, notes AP.\n"
     ]
    }
   ],
   "source": [
    "print(test_samples['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dab73333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A diamond-edged saw becomes stuck in a thick pipe on a blown-out well. Coast Guard Adm. Thad Allen says the goal is to free the saw and finish the cut later in the day. If the cut is not as smooth as engineers would like, they would be forced to put a looser fitting cap.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_before_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16cb65af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"– The Coast Guard is using a diamond-edged saw to cut through a thick pipe holding oil in the Gulf of Mexico, but it's a risky move that may backfire on them, reports the Los Angeles Times. Engineers hope to finish the cut today, but a second saw is needed to cut\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_after_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea31ed5",
   "metadata": {},
   "source": [
    "## BART on Newshead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbf34a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/273 3:46:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220529_120516-kf8umf90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/kf8umf90\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.369145154953003,\n",
       " 'eval_rouge1': 26.3,\n",
       " 'eval_rouge2': 11.27,\n",
       " 'eval_rougeL': 20.15,\n",
       " 'eval_rougeLsum': 22.98,\n",
       " 'eval_gen_len': 67.63,\n",
       " 'eval_runtime': 447.8973,\n",
       " 'eval_samples_per_second': 1.217,\n",
       " 'eval_steps_per_second': 0.61}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e9ecacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4899\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12250' max='12250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12250/12250 1:58:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.174100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.233100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.475200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.559800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.622700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>2.726100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>2.640100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>2.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>2.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>2.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.644200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>2.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>2.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>2.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.063100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>2.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>2.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>2.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>2.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>2.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>2.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>2.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>2.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>2.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>2.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>2.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.799700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.755800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.828400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.781800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.801600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.778600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.787400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.771800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.768100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.611200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.629400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.644900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.644100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-500\n",
      "Configuration saved in results\\checkpoint-500\\config.json\n",
      "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-1000\n",
      "Configuration saved in results\\checkpoint-1000\\config.json\n",
      "Model weights saved in results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-1500\n",
      "Configuration saved in results\\checkpoint-1500\\config.json\n",
      "Model weights saved in results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-2000\n",
      "Configuration saved in results\\checkpoint-2000\\config.json\n",
      "Model weights saved in results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-2500\n",
      "Configuration saved in results\\checkpoint-2500\\config.json\n",
      "Model weights saved in results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-3000\n",
      "Configuration saved in results\\checkpoint-3000\\config.json\n",
      "Model weights saved in results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-3500\n",
      "Configuration saved in results\\checkpoint-3500\\config.json\n",
      "Model weights saved in results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-4000\n",
      "Configuration saved in results\\checkpoint-4000\\config.json\n",
      "Model weights saved in results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-4500\n",
      "Configuration saved in results\\checkpoint-4500\\config.json\n",
      "Model weights saved in results\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-5000\n",
      "Configuration saved in results\\checkpoint-5000\\config.json\n",
      "Model weights saved in results\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-5500\n",
      "Configuration saved in results\\checkpoint-5500\\config.json\n",
      "Model weights saved in results\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-6000\n",
      "Configuration saved in results\\checkpoint-6000\\config.json\n",
      "Model weights saved in results\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-6500\n",
      "Configuration saved in results\\checkpoint-6500\\config.json\n",
      "Model weights saved in results\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-7000\n",
      "Configuration saved in results\\checkpoint-7000\\config.json\n",
      "Model weights saved in results\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-7500\n",
      "Configuration saved in results\\checkpoint-7500\\config.json\n",
      "Model weights saved in results\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-7500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-8000\n",
      "Configuration saved in results\\checkpoint-8000\\config.json\n",
      "Model weights saved in results\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-8500\n",
      "Configuration saved in results\\checkpoint-8500\\config.json\n",
      "Model weights saved in results\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-8500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-9000\n",
      "Configuration saved in results\\checkpoint-9000\\config.json\n",
      "Model weights saved in results\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-9000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-9500\n",
      "Configuration saved in results\\checkpoint-9500\\config.json\n",
      "Model weights saved in results\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-9500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-10000\n",
      "Configuration saved in results\\checkpoint-10000\\config.json\n",
      "Model weights saved in results\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-10500\n",
      "Configuration saved in results\\checkpoint-10500\\config.json\n",
      "Model weights saved in results\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-10500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-11000\n",
      "Configuration saved in results\\checkpoint-11000\\config.json\n",
      "Model weights saved in results\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-11000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-11500\n",
      "Configuration saved in results\\checkpoint-11500\\config.json\n",
      "Model weights saved in results\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-11500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-12000\n",
      "Configuration saved in results\\checkpoint-12000\\config.json\n",
      "Model weights saved in results\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-10500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12250, training_loss=2.265575162303691, metrics={'train_runtime': 7108.1677, 'train_samples_per_second': 3.446, 'train_steps_per_second': 1.723, 'total_flos': 1.327080680718336e+16, 'train_loss': 2.265575162303691, 'epoch': 5.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab0d06d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.370157480239868,\n",
       " 'eval_rouge1': 38.04,\n",
       " 'eval_rouge2': 26.09,\n",
       " 'eval_rougeL': 33.09,\n",
       " 'eval_rougeLsum': 34.85,\n",
       " 'eval_gen_len': 64.29,\n",
       " 'eval_runtime': 408.5014,\n",
       " 'eval_samples_per_second': 1.334,\n",
       " 'eval_steps_per_second': 0.668,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68407c57",
   "metadata": {},
   "source": [
    "## BART on Newshead titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed512baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/273 2:13:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220529_155529-1i79zb2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/1i79zb2f\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.67332124710083,\n",
       " 'eval_rouge1': 12.69,\n",
       " 'eval_rouge2': 3.67,\n",
       " 'eval_rougeL': 10.41,\n",
       " 'eval_rougeLsum': 11.51,\n",
       " 'eval_gen_len': 68.17,\n",
       " 'eval_runtime': 452.7467,\n",
       " 'eval_samples_per_second': 1.204,\n",
       " 'eval_steps_per_second': 0.603}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd25fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4899\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12250' max='12250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12250/12250 1:59:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.486700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.986600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.974800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.944300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.930300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.825900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.949100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>2.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.250300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>2.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>2.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>2.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>2.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>2.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>2.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>2.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>2.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>2.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>2.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>2.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>2.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>2.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>2.339700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>2.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.906300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.905800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.912800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.923100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.894100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.862700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.890400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.895900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.848100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.928900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.924400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.854100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.865700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.669300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.657700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.655500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-500\n",
      "Configuration saved in results\\checkpoint-500\\config.json\n",
      "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-1000\n",
      "Configuration saved in results\\checkpoint-1000\\config.json\n",
      "Model weights saved in results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-1500\n",
      "Configuration saved in results\\checkpoint-1500\\config.json\n",
      "Model weights saved in results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-2000\n",
      "Configuration saved in results\\checkpoint-2000\\config.json\n",
      "Model weights saved in results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-2500\n",
      "Configuration saved in results\\checkpoint-2500\\config.json\n",
      "Model weights saved in results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-3000\n",
      "Configuration saved in results\\checkpoint-3000\\config.json\n",
      "Model weights saved in results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-3500\n",
      "Configuration saved in results\\checkpoint-3500\\config.json\n",
      "Model weights saved in results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-4000\n",
      "Configuration saved in results\\checkpoint-4000\\config.json\n",
      "Model weights saved in results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-4500\n",
      "Configuration saved in results\\checkpoint-4500\\config.json\n",
      "Model weights saved in results\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-5000\n",
      "Configuration saved in results\\checkpoint-5000\\config.json\n",
      "Model weights saved in results\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-5500\n",
      "Configuration saved in results\\checkpoint-5500\\config.json\n",
      "Model weights saved in results\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-6000\n",
      "Configuration saved in results\\checkpoint-6000\\config.json\n",
      "Model weights saved in results\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-6500\n",
      "Configuration saved in results\\checkpoint-6500\\config.json\n",
      "Model weights saved in results\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-7000\n",
      "Configuration saved in results\\checkpoint-7000\\config.json\n",
      "Model weights saved in results\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-7500\n",
      "Configuration saved in results\\checkpoint-7500\\config.json\n",
      "Model weights saved in results\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-7500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-8000\n",
      "Configuration saved in results\\checkpoint-8000\\config.json\n",
      "Model weights saved in results\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-8500\n",
      "Configuration saved in results\\checkpoint-8500\\config.json\n",
      "Model weights saved in results\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-8500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-9000\n",
      "Configuration saved in results\\checkpoint-9000\\config.json\n",
      "Model weights saved in results\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-9000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-9500\n",
      "Configuration saved in results\\checkpoint-9500\\config.json\n",
      "Model weights saved in results\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-9500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-10000\n",
      "Configuration saved in results\\checkpoint-10000\\config.json\n",
      "Model weights saved in results\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-8500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-10500\n",
      "Configuration saved in results\\checkpoint-10500\\config.json\n",
      "Model weights saved in results\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-10500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-11000\n",
      "Configuration saved in results\\checkpoint-11000\\config.json\n",
      "Model weights saved in results\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-11000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-11500\n",
      "Configuration saved in results\\checkpoint-11500\\config.json\n",
      "Model weights saved in results\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-11500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results\\checkpoint-12000\n",
      "Configuration saved in results\\checkpoint-12000\\config.json\n",
      "Model weights saved in results\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-10500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12250, training_loss=2.590498660340601, metrics={'train_runtime': 7162.7091, 'train_samples_per_second': 3.42, 'train_steps_per_second': 1.71, 'total_flos': 1.327080680718336e+16, 'train_loss': 2.590498660340601, 'epoch': 5.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c73c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.355747699737549,\n",
       " 'eval_rouge1': 19.1,\n",
       " 'eval_rouge2': 7.04,\n",
       " 'eval_rougeL': 15.84,\n",
       " 'eval_rougeLsum': 16.17,\n",
       " 'eval_gen_len': 59.11,\n",
       " 'eval_runtime': 369.8127,\n",
       " 'eval_samples_per_second': 1.474,\n",
       " 'eval_steps_per_second': 0.738,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04199e03",
   "metadata": {},
   "source": [
    "## MBART on Multinews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2b17273",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = validation_data_txt[0] \n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bc53da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [563/563 09:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220529_214245-1lbbj77i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/1lbbj77i\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.407193660736084,\n",
       " 'eval_rouge1': 20.7,\n",
       " 'eval_rouge2': 5.9,\n",
       " 'eval_rougeL': 14.2,\n",
       " 'eval_rougeLsum': 16.1,\n",
       " 'eval_gen_len': 37.2,\n",
       " 'eval_runtime': 571.7055,\n",
       " 'eval_samples_per_second': 0.985,\n",
       " 'eval_steps_per_second': 0.985}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253bbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 563\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 563\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [563/563 1:01:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.994300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.464800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.427900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-500\n",
      "Configuration saved in results\\checkpoint-500\\config.json\n",
      "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [results\\checkpoint-11000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=563, training_loss=4.521804762140684, metrics={'train_runtime': 3713.0015, 'train_samples_per_second': 0.152, 'train_steps_per_second': 0.152, 'total_flos': 305023514836992.0, 'train_loss': 4.521804762140684, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d30f3d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 563\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [563/563 1:58:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.5758073329925537,\n",
       " 'eval_rouge1': 33.2,\n",
       " 'eval_rouge2': 12.4,\n",
       " 'eval_rougeL': 23.4,\n",
       " 'eval_rougeLsum': 27.4,\n",
       " 'eval_gen_len': 69.7,\n",
       " 'eval_runtime': 7111.3982,\n",
       " 'eval_samples_per_second': 0.079,\n",
       " 'eval_steps_per_second': 0.079,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699f090",
   "metadata": {},
   "source": [
    "## MBART on Newshead summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "479f931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [545/545 09:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220530_101243-37eftdp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/37eftdp1\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.711479663848877,\n",
       " 'eval_rouge1': 28.3,\n",
       " 'eval_rouge2': 17.2,\n",
       " 'eval_rougeL': 25.1,\n",
       " 'eval_rougeLsum': 25.3,\n",
       " 'eval_gen_len': 36.9,\n",
       " 'eval_runtime': 546.5747,\n",
       " 'eval_samples_per_second': 0.997,\n",
       " 'eval_steps_per_second': 0.997}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7d24d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 545\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 545\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220530_101614-3of621c0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/3of621c0\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [545/545 59:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.922400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-500\n",
      "Configuration saved in results\\checkpoint-500\\config.json\n",
      "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=545, training_loss=3.95895069367295, metrics={'train_runtime': 3565.6985, 'train_samples_per_second': 0.153, 'train_steps_per_second': 0.153, 'total_flos': 295271430881280.0, 'train_loss': 3.95895069367295, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9b72cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [545/545 1:05:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.1031317710876465,\n",
       " 'eval_rouge1': 32.4,\n",
       " 'eval_rouge2': 20.4,\n",
       " 'eval_rougeL': 28.8,\n",
       " 'eval_rougeLsum': 29.2,\n",
       " 'eval_gen_len': 35.2,\n",
       " 'eval_runtime': 3964.7839,\n",
       " 'eval_samples_per_second': 0.137,\n",
       " 'eval_steps_per_second': 0.137,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633cb4a6",
   "metadata": {},
   "source": [
    "## MBART on Newshead titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "094991d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [545/545 08:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmhakan20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\pc\\Desktop\\NLP_SSL_PRJ\\wandb\\run-20220530_150219-3vmqddzk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mhakan20/huggingface/runs/3vmqddzk\" target=\"_blank\">results</a></strong> to <a href=\"https://wandb.ai/mhakan20/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.749789237976074,\n",
       " 'eval_rouge1': 19.1,\n",
       " 'eval_rouge2': 6.4,\n",
       " 'eval_rougeL': 16.8,\n",
       " 'eval_rougeLsum': 17.0,\n",
       " 'eval_gen_len': 35.0,\n",
       " 'eval_runtime': 501.2462,\n",
       " 'eval_samples_per_second': 1.087,\n",
       " 'eval_steps_per_second': 1.087}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cd41500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 545\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 545\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [545/545 1:00:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.997800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results\\checkpoint-500\n",
      "Configuration saved in results\\checkpoint-500\\config.json\n",
      "Model weights saved in results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in results\\checkpoint-500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=545, training_loss=4.823710338347548, metrics={'train_runtime': 3619.4511, 'train_samples_per_second': 0.151, 'train_steps_per_second': 0.151, 'total_flos': 295271430881280.0, 'train_loss': 4.823710338347548, 'epoch': 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a002f880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 545\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [545/545 35:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.580254077911377,\n",
       " 'eval_rouge1': 30.3,\n",
       " 'eval_rouge2': 15.8,\n",
       " 'eval_rougeL': 27.6,\n",
       " 'eval_rougeLsum': 27.6,\n",
       " 'eval_gen_len': 15.5,\n",
       " 'eval_runtime': 2144.6005,\n",
       " 'eval_samples_per_second': 0.254,\n",
       " 'eval_steps_per_second': 0.254,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8a7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
